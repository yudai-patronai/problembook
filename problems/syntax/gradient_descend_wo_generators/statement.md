---
id: b277ac00-35ee-4bad-8e82-8ecd320c108d
longname: Градиентный спуск
languages: [python]
tags: [syntax]
checker: cmp_file
time_limit: 1
real_time_limit: 1
max_vm_size: 64M
---

Градиентный спуск явлется одним из самых популярных методов оптимизации -- нахождения экстремума некоторого функционала.

Для работы метода необходимо знать функцию и ее первую производную. Метод заключается в следующем: мы берем некоторое начальное приближение экстремума. Затем итеративно вычисляем следующее приближение на основе предыдущего по формуле X<sub>n+1</sub> = X<sub>n</sub> - a*f'(X<sub>n</sub>), где a -- learning rate задает скорость, с которой сходится модель. Считается, что модель сошлась, когда изменение значения функции между соседними итерациями становится меньше или равным заданному порогу &epsilon;.

Ваша задача -- написать **функцию** `def gradient_descent(func, derivative, x0, learning_rate, eps);`, которая будет искать экстремум методом градиентного спуска. Функция должна вернуть пару (x<sub>i</sub>, f(x<sub>i</sub>)) с той итерации, на которой она сойдется.

*Примечание:* func и derivative являются объектами-функциями, которые можно использовать как обычные функции, т.е.
`func(x)`.

### Формат входных данных

На вход вункции передается целевая функция, ее производная, начальное приближение, learning rate, epsilon. Последние два аргумента могут отсутствовать, тогда значение по умолчанию для learning rate -- 0.1, для epsilon -- 1e-4

### Формат выходных данных

Пара (x<sub>i</sub>, f(x<sub>i</sub>)) с последней итерации функции.

### Примеры

```
-> gradient_descent(`x*x`, `2*x`, 1)
--
<- 0.011529 0.00013292
```
