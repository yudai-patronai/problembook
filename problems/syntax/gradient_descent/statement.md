---
id: 01621cc8-1a60-4683-80eb-81cb6c0b4692
longname: Градиентный спуск
languages: [python]
tags: [syntax,generator]
checker: cmp_file
time_limit: 1
real_time_limit: 1
max_vm_size: 64M
---

Градиентный спуск явлется одним из самых популярных методов оптимизации -- нахождения экстремума некоторого функционала.

Для работы метода необходимо знать функцию и ее первую производную. Метод заключается в следующем: мы берем некоторое начальное приближение экстремума. Затем итеративно вычисляем следующее приближение на основе предыдущего по формуле X<sub>n+1</sub> = X<sub>n</sub> - a*f'(X<sub>n</sub), где a -- learning rate задает скорость, с которой сходится модель. Считается, что модель сошлась, когда изменение значения функции между соседними итерациями становится меньше или равным заданному порогу &epsilon;.

Ваша задача -- написать **функцию** `def gradient_descent(func, derivative, x0, learning_rate, eps);`, которая будет являться генератором и возвращать все пары (x<sub>i</sub>, f(x<sub>i</sub>)), i=[0,N] до сходимости метода.

### Формат входных данных

На вход вункции передается целевая функция, ее производная, начальное приближение, learning rate, epsilon. Последние два аргумента могут отсутствовать, тогда значение по умолчанию для learning rate -- 0.1, для epsilon -- 1e-4 

### Формат выходных данных

Генератор, выдающий все пары (x<sub>i</sub>, f(x<sub>i</sub>))

### Примеры

```
-> gradient_descent(x*x, 2*x, 1)
--
<- 1.0        1.0       
<- 0.8        0.64      
<- 0.64       0.4096    
<- 0.512      0.26214   
<- 0.4096     0.16777   
<- 0.32768    0.10737   
<- 0.26214    0.068719  
<- 0.20972    0.04398   
<- 0.16777    0.028147  
<- 0.13422    0.018014  
<- 0.10737    0.011529  
<- 0.085899   0.0073787 
<- 0.068719   0.0047224 
<- 0.054976   0.0030223 
<- 0.04398    0.0019343 
<- 0.035184   0.0012379 
<- 0.028147   0.00079228
<- 0.022518   0.00050706
<- 0.018014   0.00032452
<- 0.014412   0.00020769
<- 0.011529   0.00013292
```
